import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm  # tqdmæ˜¯æ˜¾ç¤ºå¾ªç¯è¿›åº¦æ¡çš„åº“


class CliffWalkingEnv:
    def __init__(self, n_col, n_row):
        self.n_col = n_col  # å®šä¹‰ç½‘æ ¼ä¸–ç•Œçš„åˆ—
        self.n_row = n_row  # å®šä¹‰ç½‘æ ¼ä¸–ç•Œçš„è¡Œ
        self.x = 0  # è®°å½•å½“å‰æ™ºèƒ½ä½“ä½ç½®çš„æ¨ªåæ ‡
        self.y = 0  # è®°å½•å½“å‰æ™ºèƒ½ä½“ä½ç½®çš„çºµåæ ‡

    def step(self, action):  # å¤–éƒ¨è°ƒç”¨è¿™ä¸ªå‡½æ•°æ¥æ”¹å˜å½“å‰ä½ç½®
        # 4ç§åŠ¨ä½œ, change[0]:ä¸Š, change[1]:ä¸‹, change[2]:å·¦, change[3]:å³ã€‚åæ ‡ç³»åŸç‚¹(0,0)
        # å®šä¹‰åœ¨å·¦ä¸Šè§’
        change = [[0, -1], [0, 1], [-1, 0], [1, 0]]
        self.x = min(self.n_col - 1, max(0, self.x + change[action][0]))
        self.y = min(self.n_row - 1, max(0, self.y + change[action][1]))
        next_state = self.y * self.n_col + self.x
        reward = -1
        done = False
        if self.y == self.n_row - 1 and self.x > 0:  # ä¸‹ä¸€ä¸ªä½ç½®åœ¨æ‚¬å´–æˆ–è€…ç›®æ ‡
            done = True
            if self.x != self.n_col - 1:
                reward = -100
        return next_state, reward, done

    def reset(self):  # å›å½’åˆå§‹çŠ¶æ€,åæ ‡è½´åŸç‚¹åœ¨å·¦ä¸Šè§’
        self.x = 0
        self.y = 0
        return self.y * self.n_col + self.x


class Sarsa:
    """ å•æ­¥Sarsaç®—æ³• """

    def __init__(self, n_col, n_row, epsilon, alpha, gamma, n_action=4):
        self.Q_table = np.zeros([n_row * n_col, n_action])  # åˆå§‹åŒ–Q(s,a)è¡¨æ ¼
        self.n_action = n_action  # åŠ¨ä½œä¸ªæ•°
        self.alpha = alpha  # å­¦ä¹ ç‡
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # epsilon-è´ªå©ªç­–ç•¥ä¸­çš„å‚æ•°

    def take_action(self, state):  # é€‰å–ä¸‹ä¸€æ­¥çš„æ“ä½œ,å…·ä½“å®ç°ä¸ºepsilon-è´ªå©ª
        if np.random.random() < self.epsilon:
            action = np.random.randint(self.n_action)
        else:
            action = np.argmax(self.Q_table[state])
        return action

    def best_action(self, state):  # ç”¨äºæ‰“å°ç­–ç•¥
        Q_max = np.max(self.Q_table[state])
        a = [0 for _ in range(self.n_action)]  # åˆ›å»ºäº†ä¸€ä¸ªåˆ—è¡¨aï¼Œå…¶ä¸­åŒ…å«self.n_actionä¸ª0å…ƒç´ 
        for i in range(self.n_action):  # è‹¥ä¸¤ä¸ªåŠ¨ä½œçš„ä»·å€¼ä¸€æ ·,éƒ½ä¼šè®°å½•ä¸‹æ¥
            if self.Q_table[state, i] == Q_max:
                a[i] = 1
        return a

    def update(self, s0, a0, r, s1, a1):
        td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]
        self.Q_table[s0, a0] += self.alpha * td_error  # è½¬åˆ°ç¥ç»ç½‘ç»œä¸­å°±æ˜¯æ¢¯åº¦


class n_step_Sarsa:
    """ å¤šæ­¥Sarsaç®—æ³• """

    def __init__(self, n, n_col, n_row, epsilon, alpha, gamma, n_action=4):
        self.Q_table = np.zeros([n_row * n_col, n_action])
        self.n_action = n_action
        self.alpha = alpha
        self.gamma = gamma
        self.epsilon = epsilon
        self.n = n  # é‡‡ç”¨næ­¥Sarsaç®—æ³•
        self.state_list = []  # ä¿å­˜ä¹‹å‰çš„çŠ¶æ€
        self.action_list = []  # ä¿å­˜ä¹‹å‰çš„åŠ¨ä½œ
        self.reward_list = []  # ä¿å­˜ä¹‹å‰çš„å¥–åŠ±

    def take_action(self, state):
        if np.random.random() < self.epsilon:
            action = np.random.randint(self.n_action)
        else:
            action = np.argmax(self.Q_table[state])
        return action

    def best_action(self, state):  # ç”¨äºæ‰“å°ç­–ç•¥
        Q_max = np.max(self.Q_table[state])
        a = [0 for _ in range(self.n_action)]
        for i in range(self.n_action):
            if self.Q_table[state, i] == Q_max:
                a[i] = 1
        return a

    def update(self, s0, a0, r, s1, a1, done):
        self.state_list.append(s0)
        self.action_list.append(a0)
        self.reward_list.append(r)
        if len(self.state_list) == self.n:  # è‹¥ä¿å­˜çš„æ•°æ®å¯ä»¥è¿›è¡Œnæ­¥æ›´æ–°
            G = self.Q_table[s1, a1]  # å¾—åˆ°Q(s_{t+n}, a_{t+n})
            for i in reversed(range(self.n)):
                G = self.gamma * G + self.reward_list[i]  # ä¸æ–­å‘å‰è®¡ç®—æ¯ä¸€æ­¥çš„å›æŠ¥
                # å¦‚æœåˆ°è¾¾ç»ˆæ­¢çŠ¶æ€,æœ€åå‡ æ­¥è™½ç„¶é•¿åº¦ä¸å¤Ÿnæ­¥,ä¹Ÿå°†å…¶è¿›è¡Œæ›´æ–°
                if done and i > 0:
                    s = self.state_list[i]
                    a = self.action_list[i]
                    self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
            s = self.state_list.pop(0)  # å°†éœ€è¦æ›´æ–°çš„çŠ¶æ€åŠ¨ä½œä»åˆ—è¡¨ä¸­åˆ é™¤,ä¸‹æ¬¡ä¸å¿…æ›´æ–°
            a = self.action_list.pop(0)
            self.reward_list.pop(0)
            # næ­¥Sarsaçš„ä¸»è¦æ›´æ–°æ­¥éª¤
            self.Q_table[s, a] += self.alpha * (G - self.Q_table[s, a])
        if done:  # å¦‚æœåˆ°è¾¾ç»ˆæ­¢çŠ¶æ€,å³å°†å¼€å§‹ä¸‹ä¸€æ¡åºåˆ—,åˆ™å°†åˆ—è¡¨å…¨æ¸…ç©º
            self.state_list = []
            self.action_list = []
            self.reward_list = []


class QLearning:
    """ Q-learningç®—æ³• """

    def __init__(self, n_col, n_row, epsilon, alpha, gamma, n_action=4):
        self.Q_table = np.zeros([n_row * n_col, n_action])  # åˆå§‹åŒ–Q(s,a)è¡¨æ ¼
        self.n_action = n_action  # åŠ¨ä½œä¸ªæ•°
        self.alpha = alpha  # å­¦ä¹ ç‡
        self.gamma = gamma  # æŠ˜æ‰£å› å­
        self.epsilon = epsilon  # epsilon-è´ªå©ªç­–ç•¥ä¸­çš„å‚æ•°

    def take_action(self, state):  # é€‰å–ä¸‹ä¸€æ­¥çš„æ“ä½œ
        if np.random.random() < self.epsilon:
            action = np.random.randint(self.n_action)
        else:
            action = np.argmax(self.Q_table[state])
        return action

    def best_action(self, state):  # ç”¨äºæ‰“å°ç­–ç•¥
        Q_max = np.max(self.Q_table[state])
        a = [0 for _ in range(self.n_action)]
        for i in range(self.n_action):
            if self.Q_table[state, i] == Q_max:
                a[i] = 1
        return a

    def update(self, s0, a0, r, s1):
        td_error = r + self.gamma * self.Q_table[s1].max() - self.Q_table[s0, a0]
        self.Q_table[s0, a0] += self.alpha * td_error


def print_agent(agent, env, action_meaning, disaster=[], end=[]):
    # disaster=[], end=[]åˆ†åˆ«å®šä¹‰çš„æ‚¬å´–çš„ä½ç½®ä»¥åŠç»ˆç‚¹çš„ä½ç½®
    for i in range(env.n_row):
        for j in range(env.n_col):
            if (i * env.n_col + j) in disaster:
                print('-_-', end=' ')
            elif (i * env.n_col + j) in end:
                print('^_^', end=' ')
            else:
                a = agent.best_action(i * env.n_col + j)
                pi_str = ''
                for k in range(len(action_meaning)):
                    pi_str += action_meaning[k] if a[k] > 0 else 'x'
                print(pi_str, end=' ')
        print()


# è°ƒç”¨å•æ­¥SARSAç®—æ³•
n_col = 12
n_row = 4
env = CliffWalkingEnv(n_col, n_row)
np.random.seed(0)
epsilon = 0.1
alpha = 0.1
gamma = 0.9
agent = Sarsa(n_col, n_row, epsilon, alpha, gamma)
num_episodes = 500  # æ™ºèƒ½ä½“åœ¨ç¯å¢ƒä¸­è¿è¡Œçš„åºåˆ—çš„æ•°é‡
return_list = []  # è®°å½•æ¯ä¸€æ¡åºåˆ—çš„å›æŠ¥

for i in range(10):  # æ˜¾ç¤º10ä¸ªè¿›åº¦æ¡
    # tqdmçš„è¿›åº¦æ¡åŠŸèƒ½
    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:
        for i_episode in range(int(num_episodes / 10)):  # æ¯ä¸ªè¿›åº¦æ¡çš„åºåˆ—æ•° è¿™é‡Œæ˜¯50ä¸ªåºåˆ—ï¼ˆè¯•éªŒï¼‰
            episode_return = 0  # æ¯è½®è¯•éªŒçš„å¥–åŠ±æ¸…é›¶
            state = env.reset()  # æ™ºèƒ½ä½“å›åˆ°èµ·ç‚¹
            action = agent.take_action(state)
            done = False
            while not done:
                next_state, reward, done = env.step(action)
                next_action = agent.take_action(next_state)
                episode_return += reward  # è¿™é‡Œå›æŠ¥çš„è®¡ç®—ä¸è¿›è¡ŒæŠ˜æ‰£å› å­è¡°å‡
                agent.update(state, action, reward, next_state, next_action)
                state = next_state
                action = next_action
            return_list.append(episode_return)
            if (i_episode + 1) % 10 == 0:  # æ¯10æ¡åºåˆ—æ‰“å°ä¸€ä¸‹è¿™10æ¡åºåˆ—çš„å¹³å‡å›æŠ¥
                pbar.set_postfix({
                    'episode':
                        '%d' % (num_episodes / 10 * i + i_episode + 1),
                    'return':
                        '%.3f' % np.mean(return_list[-10:])  # è¡¨ç¤ºä»return_listä¸­è·å–æœ€å10ä¸ªå…ƒç´ ã€‚
                    # è´Ÿç´¢å¼•-10æŒ‡çš„æ˜¯ä»åˆ—è¡¨çš„æœ«å°¾å¼€å§‹è®¡æ•°ï¼Œå› æ­¤-10:å–çš„æ˜¯æœ€å10ä¸ªå…ƒç´ 
                })
            pbar.update(1)

episodes_list = list(range(len(return_list)))
plt.plot(episodes_list, return_list)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('Sarsa on {}'.format('Cliff Walking'))
plt.minorticks_on()
plt.grid(which='minor', linestyle='--', linewidth=0.75)
plt.show()

action_meaning = ['ğŸ‘†', 'ğŸ‘‡', 'ğŸ‘ˆ', 'ğŸ‘‰']
print('\nSarsaç®—æ³•æœ€ç»ˆæ”¶æ•›å¾—åˆ°çš„ç­–ç•¥ä¸ºï¼š')
print_agent(agent, env, action_meaning, list(range(37, 47)), [47])

# è°ƒç”¨å¤šæ­¥SARSAç®—æ³•
np.random.seed(0)
n_step = 5  # 5æ­¥Sarsaç®—æ³•
agent = n_step_Sarsa(n_step, n_col, n_row, epsilon, alpha, gamma)
return_list = []  # è®°å½•æ¯ä¸€æ¡åºåˆ—çš„å›æŠ¥

for i in range(10):  # æ˜¾ç¤º10ä¸ªè¿›åº¦æ¡
    # tqdmçš„è¿›åº¦æ¡åŠŸèƒ½
    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:
        for i_episode in range(int(num_episodes / 10)):  # æ¯ä¸ªè¿›åº¦æ¡çš„åºåˆ—æ•°
            episode_return = 0
            state = env.reset()
            action = agent.take_action(state)
            done = False
            while not done:
                next_state, reward, done = env.step(action)
                next_action = agent.take_action(next_state)
                episode_return += reward  # è¿™é‡Œå›æŠ¥çš„è®¡ç®—ä¸è¿›è¡ŒæŠ˜æ‰£å› å­è¡°å‡
                agent.update(state, action, reward, next_state, next_action,
                             done)
                state = next_state
                action = next_action
            return_list.append(episode_return)
            if (i_episode + 1) % 10 == 0:  # æ¯10æ¡åºåˆ—æ‰“å°ä¸€ä¸‹è¿™10æ¡åºåˆ—çš„å¹³å‡å›æŠ¥
                pbar.set_postfix({
                    'episode':
                        '%d' % (num_episodes / 10 * i + i_episode + 1),
                    'return':
                        '%.3f' % np.mean(return_list[-10:])
                })
            pbar.update(1)

episodes_list = list(range(len(return_list)))
plt.plot(episodes_list, return_list)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('5-step Sarsa on {}'.format('Cliff Walking'))
plt.minorticks_on()
plt.grid(which='minor', linestyle='--', linewidth=0.75)
plt.show()

print('\nå¤šæ­¥Sarsaç®—æ³•æœ€ç»ˆæ”¶æ•›å¾—åˆ°çš„ç­–ç•¥ä¸ºï¼š')
print_agent(agent, env, action_meaning, list(range(37, 47)), [47])

# è°ƒç”¨Q-learningç®—æ³•
np.random.seed(0)
agent = QLearning(n_col, n_row, epsilon, alpha, gamma)
return_list = []  # è®°å½•æ¯ä¸€æ¡åºåˆ—çš„å›æŠ¥
for i in range(10):  # æ˜¾ç¤º10ä¸ªè¿›åº¦æ¡
    # tqdmçš„è¿›åº¦æ¡åŠŸèƒ½
    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:
        for i_episode in range(int(num_episodes / 10)):  # æ¯ä¸ªè¿›åº¦æ¡çš„åºåˆ—æ•°
            episode_return = 0
            state = env.reset()
            done = False
            while not done:
                action = agent.take_action(state)
                next_state, reward, done = env.step(action)
                episode_return += reward  # è¿™é‡Œå›æŠ¥çš„è®¡ç®—ä¸è¿›è¡ŒæŠ˜æ‰£å› å­è¡°å‡
                agent.update(state, action, reward, next_state)
                state = next_state
            return_list.append(episode_return)
            if (i_episode + 1) % 10 == 0:  # æ¯10æ¡åºåˆ—æ‰“å°ä¸€ä¸‹è¿™10æ¡åºåˆ—çš„å¹³å‡å›æŠ¥
                pbar.set_postfix({
                    'episode':
                        '%d' % (num_episodes / 10 * i + i_episode + 1),
                    'return':
                        '%.3f' % np.mean(return_list[-10:])
                })
            pbar.update(1)

episodes_list = list(range(len(return_list)))
plt.plot(episodes_list, return_list)
plt.xlabel('Episodes')
plt.ylabel('Returns')
plt.title('Q-learning on {}'.format('Cliff Walking'))
plt.minorticks_on()
plt.grid(which='minor', linestyle='--', linewidth=0.75)
plt.show()

print('Q-learningç®—æ³•æœ€ç»ˆæ”¶æ•›å¾—åˆ°çš„ç­–ç•¥ä¸ºï¼š')
print_agent(agent, env, action_meaning, list(range(37, 47)), [47])
